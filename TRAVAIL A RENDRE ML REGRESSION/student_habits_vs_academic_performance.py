# -*- coding: utf-8 -*-
"""Student Habits vs Academic Performance.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MW7Gj_vYOOD5GjPZISZKc604qKBQBzsd

**Student Habits vs Academic Performance**

Cette étude simulée, basée sur 1 000 enregistrements d’étudiants, analyse l’impact des habitudes quotidiennes — telles que les heures d’étude, le sommeil, l’usage des réseaux sociaux, la qualité de l’alimentation ou encore l’état de santé mentale — sur leurs performances académiques, mesurées par la note finale à l’examen. Deux approches sont envisagées : une régression, visant à prédire la note finale continue, et une classification, permettant de déterminer si l’étudiant réussit ou non selon un seuil fixé (par défaut la médiane).

importer les librairies necessaires:


*   `pandas` pour manipuler de donnée
*   `numpy` pour galculer numériques
*   `matplotlib.pyplot` pour faire des graphiques simples
*   `seaborn` pour faire des graphiques plus stylés et analysés
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("jayaantanaath/student-habits-vs-academic-performance")

print("Path to dataset files:", path)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

from sklearn.linear_model import LinearRegression, Lasso, LogisticRegression
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report

import joblib

"""## telechargement des donnees"""

pip install kagglehub

import kagglehub

# Download latest version
path = kagglehub.dataset_download("jayaantanaath/student-habits-vs-academic-performance")

print("Path to dataset files:", path)

import os
os.listdir()

"""## correction des types et nettoyage"""

import os
os.listdir()

from google.colab import files
uploaded = files.upload()

import os
os.listdir()

import pandas as pd
df = pd.read_csv("student_habits_performance.csv")

print("Valeurs manquantes :")
print(df.isnull().sum())

num_cols = [
    'study_hours_per_day', 'sleep_hours', 'social_media_hours',
    'exercise_frequency',
    'attendance_percentage', 'exam_score'
]

# Handle 'extracurricular_participation' separately if it has missing values
# For now, we'll assume it's categorical and will be handled later if needed.

for col in num_cols:
    if df[col].dtype == 'object': # Check if column is object type (string/categorical)
        print(f"Warning: Column '{col}' is not numeric. Skipping median imputation.")
    else:
        df[col] = df[col].fillna(df[col].median())

# Impute missing values for 'parental_education_level' (categorical) using the mode
if 'parental_education_level' in df.columns:
    df['parental_education_level'] = df['parental_education_level'].fillna(df['parental_education_level'].mode()[0])

# For 'extracurricular_participation', if it has missing values, impute with mode
if 'extracurricular_participation' in df.columns and df['extracurricular_participation'].isnull().any():
    df['extracurricular_participation'] = df['extracurricular_participation'].fillna(df['extracurricular_participation'].mode()[0])

ordinal_cols = ['diet_quality', 'mental_health_rating']

for col in ordinal_cols:
    df[col] = df[col].fillna(df[col].mode()[0])

cat_cols = ['part_time_job']

for col in cat_cols:
    df[col] = df[col].fillna(df[col].mode()[0])

print("Valeurs manquantes après nettoyage :")
print(df.isnull().sum())

"""# analyse exploratoire des données.

Statistiques descriptives
"""

display(df.describe().T)

"""Distribution de la note finale"""

plt.figure(figsize=(8,4))
sns.histplot(df['exam_score'], kde=True, bins=20)
plt.title("Distribution des notes finales")
plt.xlabel("Note finale")
plt.show()

"""Distribution de la réussite (classification)"""

median_score = df['exam_score'].median()
df['passed'] = df['exam_score'] >= median_score

plt.figure(figsize=(6,4))
sns.countplot(x='passed', data=df)
plt.title("Répartition réussite/échec")
plt.show()

"""Distribution de la note finale"""

plt.figure(figsize=(12,8))
sns.heatmap(df[num_cols].corr(), annot=True, cmap="coolwarm")
plt.title("Matrice de corrélation des variables numériques")
plt.show()

"""Boxplots pour détecter outliers"""

for col in num_cols[:-1]:  # exclure FinalScore
    plt.figure(figsize=(6,4))
    sns.boxplot(y=df[col])
    plt.title(f"Boxplot : {col}")
    plt.show()

"""## Préparer les features et la cible

Features (toutes sauf FinalScore et passed)
"""

features = [c for c in df.columns if c not in ['student_id', 'exam_score','passed']]
X = df[features]
y_reg = df['exam_score']  # Régression
y_clf = df['passed']      # Classification

"""Identifier types"""

num_features = X.select_dtypes(include=[np.number]).columns.tolist()
cat_features = [c for c in features if c not in num_features]
print("Numériques :", num_features)
print("Catégorielles :", cat_features)

"""## Pipeline de preprocessing"""

from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

# Import nécessaire pour le pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(transformers=[
    ('num', numeric_transformer, num_features),
    ('cat', categorical_transformer, cat_features)
])

"""## Régression : prédire la note finale"""

# Pour séparer train/test
from sklearn.model_selection import train_test_split, GridSearchCV

# Pour créer le pipeline
from sklearn.pipeline import Pipeline

# Pour le modèle
from sklearn.ensemble import RandomForestRegressor

# Pour le prétraitement si tu utilises un ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer

# Pour l’évaluation
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Pour les calculs numériques
import numpy as np

X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X, y_reg, test_size=0.2, random_state=42)

pipe_rf_reg = Pipeline(steps=[
    ('preproc', preprocessor),
    ('model', RandomForestRegressor(random_state=42))
])

param_grid_reg = {
    'model__n_estimators': [100, 200],
    'model__max_depth': [None, 6, 12],
    'model__min_samples_split': [2, 5]
}

gs_reg = GridSearchCV(pipe_rf_reg, param_grid_reg, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)
gs_reg.fit(X_train_r, y_train_r)

print("Meilleurs paramètres :", gs_reg.best_params_)

# Évaluation
best_reg = gs_reg.best_estimator_
y_pred_r = best_reg.predict(X_test_r)

mae = mean_absolute_error(y_test_r, y_pred_r)
rmse = np.sqrt(mean_squared_error(y_test_r, y_pred_r))
r2 = r2_score(y_test_r, y_pred_r)
print(f"Régression — MAE : {mae:.2f}, RMSE : {rmse:.2f}, R² : {r2:.2f}")

# Récupérer noms features après OHE
ohe = best_reg.named_steps['preproc'].named_transformers_['cat'].named_steps['onehot']
ohe_names = ohe.get_feature_names_out(cat_features)
all_features = num_features + list(ohe_names)

importances = best_reg.named_steps['model'].feature_importances_
feat_imp = pd.Series(importances, index=all_features).sort_values(ascending=False).head(20)

plt.figure(figsize=(8,6))
sns.barplot(x=feat_imp.values, y=feat_imp.index)
plt.title("Top 20 features importantes (Régression)")
plt.show()

"""## Classification : prédire réussite/échec"""

# Pour le pipeline
from sklearn.pipeline import Pipeline

# Pour le prétraitement
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer

# Pour la séparation train/test et GridSearch
from sklearn.model_selection import train_test_split, GridSearchCV

# Pour le modèle
from sklearn.ensemble import RandomForestClassifier

# Pour l'évaluation
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Pour les calculs et visualisation
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X, y_clf, test_size=0.2, random_state=42)

pipe_rf_clf = Pipeline(steps=[
    ('preproc', preprocessor),
    ('model', RandomForestClassifier(random_state=42))
])

param_grid_clf = {
    'model__n_estimators': [100, 200],
    'model__max_depth': [None, 8],
    'model__class_weight': [None, 'balanced']
}

gs_clf = GridSearchCV(pipe_rf_clf, param_grid_clf, cv=5, scoring='f1', n_jobs=-1)
gs_clf.fit(X_train_c, y_train_c)

best_clf = gs_clf.best_estimator_
y_pred_c = best_clf.predict(X_test_c)

print("Classification — Accuracy :", accuracy_score(y_test_c, y_pred_c))
print(classification_report(y_test_c, y_pred_c))

# Matrice de confusion
cm = confusion_matrix(y_test_c, y_pred_c)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Prédit")
plt.ylabel("Réel")
plt.title("Matrice de confusion")
plt.show()

"""## Sauvegarde des modèles

"""

import joblib

# Sauvegarder les modèles
joblib.dump(best_reg, "best_rf_regressor.pkl")
joblib.dump(best_clf, "best_rf_classifier.pkl")
print("Modèles sauvegardés !")

joblib.dump(best_reg, "best_rf_regressor.pkl")
joblib.dump(best_clf, "best_rf_classifier.pkl")
print("Modèles sauvegardés !")