# -*- coding: utf-8 -*-
"""E-commerce Sales Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zJ_X2Se-ntJLnJRvxJDK2J_FGhPmMhQo

## Introduction
# ðŸ“Œ E-Commerce Sales Prediction â€” Machine Learning Project

## ðŸŽ¯ Objectif du projet
Lâ€™objectif de ce projet est de prÃ©dire le montant des ventes rÃ©alisÃ©es par un site e-commerce Ã  partir de diffÃ©rentes caractÃ©ristiques (prix, catÃ©gorie, rating, rÃ©ductionâ€¦).

Ce travail s'inscrit dans le cadre du module *Data Science & Machine Learning* et suit les Ã©tapes complÃ¨tes dâ€™un projet de ML :
- Analyse du dataset
- Nettoyage & prÃ©paration
- Exploration des donnÃ©es (EDA)
- Feature Engineering
- ModÃ©lisation et Ã©valuation
- InterprÃ©tation finale

## ðŸ” ProblÃ©matique
La variable Ã  prÃ©dire est **Sales**, une variable numÃ©rique continue.

âž¡ï¸ Il sâ€™agit donc dâ€™un **problÃ¨me de rÃ©gression supervisÃ©e**.

## Importation des librairies
"""

# Commented out IPython magic to ensure Python compatibility.
# Manipulation des donnÃ©es
import pandas as pd
import numpy as np

# Visualisation
import matplotlib.pyplot as plt
import seaborn as sns
plt.style.use('seaborn-v0_8-darkgrid')
# %matplotlib inline

# Preprocessing et Machine Learning
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Autres
import warnings
warnings.filterwarnings('ignore')

# Configuration des visualisations
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (12, 6)
plt.rcParams['font.size'] = 10

print("âœ… Toutes les bibliothÃ¨ques ont Ã©tÃ© importÃ©es avec succÃ¨s!")

"""## Chargement du dataset"""

from google.colab import files

# Upload du fichier
print("ðŸ“ Veuillez sÃ©lectionner votre fichier CSV...")
uploaded = files.upload()

# Charger le dataset
df = pd.read_csv('Ecommerce_Sales_Prediction_Dataset.csv')

# Afficher les informations de base
print("\n" + "=" * 70)
print("INFORMATIONS SUR LE DATASET")
print("=" * 70)
print(f"\nNombre de lignes: {df.shape[0]}")
print(f"Nombre de colonnes: {df.shape[1]}")
print(f"\nNom des colonnes:\n{df.columns.tolist()}")

print("\n" + "=" * 70)
print("APERÃ‡U DES DONNÃ‰ES (5 premiÃ¨res lignes)")
print("=" * 70)
print(df.head())

print("\n" + "=" * 70)
print("TYPES DE DONNÃ‰ES")
print("=" * 70)
print(df.dtypes)

"""# ANALYSE EXPLORATOIRE DES DONNÃ‰ES (EDA)"""

# VÃ©rifier les valeurs manquantes
print("=" * 70)
print("VALEURS MANQUANTES")
print("=" * 70)
missing = df.isnull().sum()
if missing.sum() == 0:
    print("âœ… Aucune valeur manquante dÃ©tectÃ©e!")
else:
    print(missing[missing > 0])

print("\n" + "=" * 70)
print("VALEURS UNIQUES PAR COLONNE")
print("=" * 70)
for col in df.columns:
    print(f"{col:20} : {df[col].nunique():5} valeurs uniques")

print("\n" + "=" * 70)
print("STATISTIQUES DESCRIPTIVES")
print("=" * 70)
print(df.describe())

"""DISTRIBUTION DES VARIABLES NUMÃ‰RIQUES"""

# CrÃ©er la colonne Sales (Revenu Total = Prix Ã— UnitÃ©s Vendues)
df['Sales'] = df['Price'] * df['Units_Sold']

print("âœ… Colonne 'Sales' crÃ©Ã©e avec succÃ¨s!")
print("\n" + "=" * 70)
print("STATISTIQUES DE SALES")
print("=" * 70)
print(f"Revenu total: ${df['Sales'].sum():,.2f}")
print(f"Revenu moyen par transaction: ${df['Sales'].mean():,.2f}")
print(f"Revenu mÃ©dian: ${df['Sales'].median():,.2f}")
print(f"Revenu minimum: ${df['Sales'].min():,.2f}")
print(f"Revenu maximum: ${df['Sales'].max():,.2f}")

# Distribution des variables numÃ©riques
numeric_cols = ['Price', 'Discount', 'Marketing_Spend', 'Units_Sold', 'Sales']

fig, axes = plt.subplots(2, 3, figsize=(18, 10))
axes = axes.flatten()

for i, col in enumerate(numeric_cols):
    axes[i].hist(df[col], bins=50, edgecolor='black', alpha=0.7, color='steelblue')
    axes[i].set_title(f'Distribution de {col}', fontsize=14, fontweight='bold')
    axes[i].set_xlabel(col)
    axes[i].set_ylabel('FrÃ©quence')
    axes[i].grid(alpha=0.3)

# Supprimer le dernier subplot vide
fig.delaxes(axes[5])

plt.tight_layout()
plt.show()

print("âœ… Graphique des distributions crÃ©Ã©!")

"""# commentaire
Les histogrammes montrent que toutes les variables sont bien distribuÃ©es et Ã©quilibrÃ©es. Le prix varie uniformÃ©ment de 0 Ã  1000$, les rÃ©ductions de 0 Ã  50%, et le marketing de 0 Ã  10 000$. La variable cible (Units_Sold) suit une distribution normale centrÃ©e Ã  30 unitÃ©s, ce qui est parfait pour le machine learning. Le revenu total montre le pattern classique : beaucoup de petites ventes et quelques gros revenus. Les donnÃ©es sont de trÃ¨s bonne qualitÃ© et sans biais.

BOXPLOTS POUR DÃ‰TECTER LES OUTLIERS
"""

# Boxplots pour dÃ©tecter les outliers
numeric_cols = ['Price', 'Discount', 'Marketing_Spend', 'Units_Sold', 'Sales']

fig, axes = plt.subplots(2, 3, figsize=(18, 10))
axes = axes.flatten()

for i, col in enumerate(numeric_cols):
    axes[i].boxplot(df[col], vert=True, patch_artist=True,
                    boxprops=dict(facecolor='lightblue', alpha=0.7),
                    medianprops=dict(color='red', linewidth=2))
    axes[i].set_title(f'Boxplot de {col}', fontsize=14, fontweight='bold')
    axes[i].set_ylabel(col)
    axes[i].grid(alpha=0.3, axis='y')

# Supprimer le dernier subplot vide
fig.delaxes(axes[5])

plt.tight_layout()
plt.show()

print("âœ… Boxplots crÃ©Ã©s!")

"""# commentaire
Les boxplots rÃ©vÃ¨lent la prÃ©sence d'outliers (valeurs aberrantes) dans Units_Sold et Sales, visibles par les points rouges au-dessus des moustaches. Pour Price, Discount et Marketing_Spend, les donnÃ©es sont bien centrÃ©es sans outliers significatifs. Les boÃ®tes bleues reprÃ©sentent 50% des donnÃ©es (du 25e au 75e percentile), la ligne rouge est la mÃ©diane. Bien que quelques valeurs extrÃªmes existent dans Units_Sold et Sales, elles sont acceptables et reflÃ¨tent des transactions rÃ©elles. Les donnÃ©es restent de bonne qualitÃ© pour la modÃ©lisation.

DISTRIBUTION DES CATÃ‰GORIES DE PRODUITS
"""

# Distribution des catÃ©gories de produits
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Graphique en barres
df['Product_Category'].value_counts().plot(kind='bar', ax=axes[0],
                                            color='skyblue', edgecolor='black', linewidth=2)
axes[0].set_title('Distribution des CatÃ©gories de Produits', fontsize=14, fontweight='bold')
axes[0].set_xlabel('CatÃ©gorie de Produit')
axes[0].set_ylabel('Nombre de Transactions')
axes[0].tick_params(axis='x', rotation=45)
axes[0].grid(alpha=0.3, axis='y')

# Graphique en camembert (pie chart)
colors = plt.cm.Set3(range(len(df['Product_Category'].unique())))
df['Product_Category'].value_counts().plot(kind='pie', ax=axes[1],
                                            autopct='%1.1f%%', startangle=90,
                                            colors=colors)
axes[1].set_title('RÃ©partition des CatÃ©gories de Produits (%)', fontsize=14, fontweight='bold')
axes[1].set_ylabel('')

plt.tight_layout()
plt.show()

print("=" * 70)
print("NOMBRE DE TRANSACTIONS PAR CATÃ‰GORIE")
print("=" * 70)
print(df['Product_Category'].value_counts())

"""# commentaire
Les 5 catÃ©gories sont Ã©quilibrÃ©es avec environ 190-210 transactions chacune (19-21% du total). Electronics domine lÃ©gÃ¨rement avec 21%, suivi de Fashion et Toys Ã  19-20%, puis Sports et Home Decor Ã  20-21%. Cet Ã©quilibre parfait signifie qu'aucune catÃ©gorie ne domine le dataset, ce qui garantit de bonnes prÃ©dictions pour tous les produits. Pas de biais vers une catÃ©gorie particuliÃ¨re.

## matrice de correlation
"""

# Matrice de corrÃ©lation
corr_cols = ['Price', 'Discount', 'Marketing_Spend', 'Units_Sold', 'Sales']
correlation_matrix = df[corr_cols].corr()

# CrÃ©er la heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm',
            square=True, linewidths=2, cbar_kws={"shrink": 0.8},
            vmin=-1, vmax=1, center=0)
plt.title('Matrice de CorrÃ©lation entre les Variables', fontsize=16, fontweight='bold')
plt.tight_layout()
plt.show()

print("=" * 70)
print("CORRÃ‰LATIONS AVEC UNITS_SOLD (Variable Cible)")
print("=" * 70)
correlations = correlation_matrix['Units_Sold'].sort_values(ascending=False)
print(correlations)

"""# commentaire
La matrice rÃ©vÃ¨le les relations clÃ©s : Price et Sales ont une corrÃ©lation trÃ¨s forte (0.90), logique puisque Sales = Price Ã— Units_Sold. Plus important, Units_Sold (notre variable cible) montre une corrÃ©lation positive modÃ©rÃ©e avec Sales (0.39) mais trÃ¨s faible avec Price (0.02), Discount (âˆ’0.03) et Marketing_Spend (0.01). Cela signifie que les variables indÃ©pendantes ont peu de corrÃ©lation linÃ©aire avec Units_Sold, ce qui requiert des modÃ¨les plus complexes comme Random Forest pour capturer les relations non-linÃ©aires. Aucune multicolinÃ©aritÃ© problÃ©matique dÃ©tectÃ©e.

SCATTER PLOTS (Relations entre Variables)
"""

# Scatter plots : Relations entre variables
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# Price vs Units_Sold
axes[0, 0].scatter(df['Price'], df['Units_Sold'], alpha=0.5, color='blue', edgecolor='black')
axes[0, 0].set_title('Prix vs UnitÃ©s Vendues', fontsize=14, fontweight='bold')
axes[0, 0].set_xlabel('Prix ($)')
axes[0, 0].set_ylabel('UnitÃ©s Vendues')
axes[0, 0].grid(alpha=0.3)

# Discount vs Units_Sold
axes[0, 1].scatter(df['Discount'], df['Units_Sold'], alpha=0.5, color='green', edgecolor='black')
axes[0, 1].set_title('RÃ©duction vs UnitÃ©s Vendues', fontsize=14, fontweight='bold')
axes[0, 1].set_xlabel('RÃ©duction (%)')
axes[0, 1].set_ylabel('UnitÃ©s Vendues')
axes[0, 1].grid(alpha=0.3)

# Marketing_Spend vs Units_Sold
axes[1, 0].scatter(df['Marketing_Spend'], df['Units_Sold'], alpha=0.5, color='red', edgecolor='black')
axes[1, 0].set_title('DÃ©penses Marketing vs UnitÃ©s Vendues', fontsize=14, fontweight='bold')
axes[1, 0].set_xlabel('DÃ©penses Marketing ($)')
axes[1, 0].set_ylabel('UnitÃ©s Vendues')
axes[1, 0].grid(alpha=0.3)

# Price vs Sales
axes[1, 1].scatter(df['Price'], df['Sales'], alpha=0.5, color='purple', edgecolor='black')
axes[1, 1].set_title('Prix vs Revenu Total', fontsize=14, fontweight='bold')
axes[1, 1].set_xlabel('Prix ($)')
axes[1, 1].set_ylabel('Revenu Total ($)')
axes[1, 1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print("âœ… Scatter plots crÃ©Ã©s!")

"""## commerntaire
Les quatre graphiques montrent les relations clÃ©s : Prix vs Units_Sold affiche un nuage de points diffus sans tendance linÃ©aire claire, confirmant la faible corrÃ©lation (0.02). RÃ©duction vs Units_Sold montre un pattern similaire, pas de relation Ã©vidente. DÃ©penses Marketing vs Units_Sold rÃ©vÃ¨le aussi un nuage dispersÃ©, sans corrÃ©lation forte visible. En revanche, Prix vs Revenu Total montre une relation linÃ©aire trÃ¨s forte et positive (0.90), ce qui est attendu mathÃ©matiquement. Ces rÃ©sultats justifient l'utilisation de modÃ¨les non-linÃ©aires comme Random Forest et Gradient Boosting pour capturer les patterns complexes.

VENTES PAR CATÃ‰GORIE ET SEGMENT CLIENT
"""

# Ventes moyennes par catÃ©gorie et segment
fig, axes = plt.subplots(2, 2, figsize=(18, 12))

# Sales par catÃ©gorie
category_sales = df.groupby('Product_Category')['Sales'].mean().sort_values(ascending=False)
axes[0, 0].bar(category_sales.index, category_sales.values, color='teal', edgecolor='black', linewidth=2)
axes[0, 0].set_title('Revenu Moyen par CatÃ©gorie de Produit', fontsize=14, fontweight='bold')
axes[0, 0].set_xlabel('CatÃ©gorie')
axes[0, 0].set_ylabel('Revenu Moyen ($)')
axes[0, 0].tick_params(axis='x', rotation=45)
axes[0, 0].grid(alpha=0.3, axis='y')

# Units_Sold par catÃ©gorie
category_units = df.groupby('Product_Category')['Units_Sold'].mean().sort_values(ascending=False)
axes[0, 1].bar(category_units.index, category_units.values, color='orange', edgecolor='black', linewidth=2)
axes[0, 1].set_title('UnitÃ©s Vendues Moyennes par CatÃ©gorie', fontsize=14, fontweight='bold')
axes[0, 1].set_xlabel('CatÃ©gorie')
axes[0, 1].set_ylabel('UnitÃ©s Vendues Moyennes')
axes[0, 1].tick_params(axis='x', rotation=45)
axes[0, 1].grid(alpha=0.3, axis='y')

# Sales par segment client
segment_sales = df.groupby('Customer_Segment')['Sales'].mean().sort_values(ascending=False)
axes[1, 0].bar(segment_sales.index, segment_sales.values, color='purple', edgecolor='black', linewidth=2)
axes[1, 0].set_title('Revenu Moyen par Segment Client', fontsize=14, fontweight='bold')
axes[1, 0].set_xlabel('Segment Client')
axes[1, 0].set_ylabel('Revenu Moyen ($)')
axes[1, 0].tick_params(axis='x', rotation=45)
axes[1, 0].grid(alpha=0.3, axis='y')

# Units_Sold par segment
segment_units = df.groupby('Customer_Segment')['Units_Sold'].mean().sort_values(ascending=False)
axes[1, 1].bar(segment_units.index, segment_units.values, color='green', edgecolor='black', linewidth=2)
axes[1, 1].set_title('UnitÃ©s Vendues Moyennes par Segment', fontsize=14, fontweight='bold')
axes[1, 1].set_xlabel('Segment Client')
axes[1, 1].set_ylabel('UnitÃ©s Vendues Moyennes')
axes[1, 1].tick_params(axis='x', rotation=45)
axes[1, 1].grid(alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

print("âœ… Graphiques de ventes par catÃ©gorie et segment crÃ©Ã©s!")

"""## commentaire
Les catÃ©gories affichent des revenus moyens quasi-identiques (15000-16000$), avec Fashion et Sports lÃ©gÃ¨rement devant. Les unitÃ©s vendues sont aussi uniformes (~29 unitÃ©s par catÃ©gorie), indiquant une performance Ã©quilibrÃ©e. Pour les segments clients, Premium gÃ©nÃ¨re 15500$ en revenu moyen avec 30 unitÃ©s vendues, lÃ©gÃ¨rement supÃ©rieur Ã  Regular (14800$) et Occasional (14700$). Les diffÃ©rences sont minimes mais consistent. Conclusion : aucune catÃ©gorie ou segment ne surperforme drastiquement. Focus recommandÃ© sur l'optimisation globale plutÃ´t que favoriser une catÃ©gorie spÃ©cifique.

TENDANCES TEMPORELLES
"""

# Tendances temporelles - Ventes mensuelles
# Convertir la colonne 'Date' en datetime
df['Date'] = pd.to_datetime(df['Date'], format='%d-%m-%Y')

# Extraire l'annÃ©e et le mois
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month

monthly_sales = df.groupby(['Year', 'Month'])['Sales'].sum().reset_index()
monthly_sales['Period'] = pd.to_datetime(monthly_sales[['Year', 'Month']].assign(DAY=1))

plt.figure(figsize=(16, 6))
plt.plot(monthly_sales['Period'], monthly_sales['Sales'], marker='o', linewidth=2.5,
         markersize=8, color='darkblue')
plt.fill_between(monthly_sales['Period'], monthly_sales['Sales'], alpha=0.3, color='lightblue')
plt.title('Ã‰volution des Ventes Mensuelles', fontsize=16, fontweight='bold')
plt.xlabel('PÃ©riode')
plt.ylabel('Ventes Totales ($)')
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

print("=" * 70)
print("VENTES TOTALES PAR MOIS")
print("=" * 70)
print(monthly_sales.to_string(index=False))

"""## commentaire
Lâ€™Ã©volution mensuelle des ventes reste globalement stable entre 400 000 et 550 000 dollars, sans chute brutale ni tendance dÃ©croissante marquÃ©e. On observe toutefois plusieurs pics rÃ©guliers oÃ¹ les ventes dÃ©passent 520 000 dollars, ce qui suggÃ¨re des pÃ©riodes plus fortes (probablement liÃ©es Ã  des campagnes marketing ou Ã  la saisonnalitÃ©). Globalement, lâ€™activitÃ© eâ€‘commerce est saine et relativement constante dans le temps, ce qui facilite la prÃ©vision et la planification des stocks.

# PRÃ‰PARATION DES DONNÃ‰ES POUR LE MACHINE LEARNING :

ENCODAGE DES VARIABLES CATÃ‰GORIELLES
"""

# Copie du dataframe pour le ML
df_ml = df.copy()

# Label Encoding pour les variables catÃ©gorielles
le_category = LabelEncoder()
le_segment = LabelEncoder()

df_ml['Product_Category_Encoded'] = le_category.fit_transform(df_ml['Product_Category'])
df_ml['Customer_Segment_Encoded'] = le_segment.fit_transform(df_ml['Customer_Segment'])

print("âœ… Encodage terminÃ©!")
print("\n" + "=" * 70)
print("MAPPING DES CATÃ‰GORIES")
print("=" * 70)
print("\nProduct_Category:")
for i, cat in enumerate(le_category.classes_):
    print(f"  {cat} -> {i}")

print("\nCustomer_Segment:")
for i, seg in enumerate(le_segment.classes_):
    print(f"  {seg} -> {i}")

print("\n" + "=" * 70)
print("APERÃ‡U DES DONNÃ‰ES ENCODÃ‰ES")
print("=" * 70)
print(df_ml[['Product_Category', 'Product_Category_Encoded', 'Customer_Segment', 'Customer_Segment_Encoded']].head(10))

"""SÃ‰LECTION DES FEATURES"""

# Copie du dataframe pour le ML
df_ml = df.copy()

# Label Encoding pour les variables catÃ©gorielles
le_category = LabelEncoder()
le_segment = LabelEncoder()

df_ml['Product_Category_Encoded'] = le_category.fit_transform(df_ml['Product_Category'])
df_ml['Customer_Segment_Encoded'] = le_segment.fit_transform(df_ml['Customer_Segment'])

# Convertir la colonne 'Date' en datetime si ce n'est pas dÃ©jÃ  fait pour df_ml
df_ml['Date'] = pd.to_datetime(df_ml['Date'], format='%d-%m-%Y', errors='coerce')

# Extraire les caractÃ©ristiques temporelles de la date
df_ml['Year'] = df_ml['Date'].dt.year
df_ml['Month'] = df_ml['Date'].dt.month
df_ml['Day'] = df_ml['Date'].dt.day
df_ml['DayOfWeek'] = df_ml['Date'].dt.dayofweek
df_ml['Quarter'] = df_ml['Date'].dt.quarter

print("âœ… Encodage et ajout de caractÃ©ristiques temporelles terminÃ©s!")
print("\n" + "=" * 70)
print("MAPPING DES CATÃ‰GORIES")
print("=" * 70)
print("\nProduct_Category:")
for i, cat in enumerate(le_category.classes_):
    print(f"  {cat} -> {i}")

print("\nCustomer_Segment:")
for i, seg in enumerate(le_segment.classes_):
    print(f"  {seg} -> {i}")

print("\n" + "=" * 70)
print("APERÃ‡U DES DONNÃ‰ES ENCODÃ‰ES ET AVEC FEATURES TEMPORELLES")
print("=" * 70)
print(df_ml[['Product_Category', 'Product_Category_Encoded', 'Customer_Segment', 'Customer_Segment_Encoded', 'Date', 'Year', 'Month', 'Day', 'DayOfWeek', 'Quarter']].head(10))

# Features pour prÃ©dire Units_Sold
feature_columns = [
    'Price',
    'Discount',
    'Marketing_Spend',
    'Product_Category_Encoded',
    'Customer_Segment_Encoded',
    'Month',
    'DayOfWeek',
    'Quarter'
]

X = df_ml[feature_columns]
y = df_ml['Units_Sold']

print("\n" + "=" * 70)
print("FEATURES SÃ‰LECTIONNÃ‰ES POUR LE MODÃˆLE")
print("=" * 70)
print(f"\nNombre de features: {X.shape[1]}")
print(f"Nombre d'observations: {X.shape[0]}")
print(f"\nListe des features:")
for i, col in enumerate(feature_columns, 1):
    print(f"  {i}. {col}")

print("\n" + "=" * 70)
print("VARIABLE CIBLE: Units_Sold")
print("=" * 70)
print(f"Moyenne: {y.mean():.2f}")
print(f"MÃ©diane: {y.median():.2f}")
print(f"Min: {y.min()}")
print(f"Max: {y.max()}")

print("\n" + "=" * 70)
print("APERÃ‡U DES FEATURES")
print("=" * 70)
print(X.head())

"""SPLIT TRAIN/TEST"""

# Division des donnÃ©es en train et test (80/20)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("=" * 70)
print("DIVISION DES DONNÃ‰ES")
print("=" * 70)
print(f"\nTaille du jeu d'entraÃ®nement: {X_train.shape[0]} lignes ({X_train.shape[0]/len(X)*100:.1f}%)")
print(f"Taille du jeu de test: {X_test.shape[0]} lignes ({X_test.shape[0]/len(X)*100:.1f}%)")

print("\n" + "=" * 70)
print("STATISTIQUES - JEU D'ENTRAÃŽNEMENT")
print("=" * 70)
print(f"y_train - Moyenne: {y_train.mean():.2f}")
print(f"y_train - MÃ©diane: {y_train.median():.2f}")
print(f"y_train - Min: {y_train.min()}, Max: {y_train.max()}")

print("\n" + "=" * 70)
print("STATISTIQUES - JEU DE TEST")
print("=" * 70)
print(f"y_test - Moyenne: {y_test.mean():.2f}")
print(f"y_test - MÃ©diane: {y_test.median():.2f}")
print(f"y_test - Min: {y_test.min()}, Max: {y_test.max()}")

"""STANDARDISATION (SCALING)"""

# Standardisation des features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("âœ… Standardisation terminÃ©e!")
print("\n" + "=" * 70)
print("VÃ‰RIFICATION DE LA STANDARDISATION")
print("=" * 70)

print("\nMoyennes aprÃ¨s scaling (train set):")
print(X_train_scaled.mean(axis=0).round(2))

print("\nÃ‰cart-types aprÃ¨s scaling (train set):")
print(X_train_scaled.std(axis=0).round(2))

print("\n" + "=" * 70)
print("AVANT et APRÃˆS SCALING")
print("=" * 70)
print("\nAVANT (X_train original):")
print(X_train.describe().T[['mean', 'std']])

print("\nAPRÃˆS (X_train_scaled):")
df_scaled_stats = pd.DataFrame(X_train_scaled, columns=feature_columns)
print(df_scaled_stats.describe().T[['mean', 'std']])

# Convertir en DataFrame pour voir plus clairement
X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=feature_columns)
X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=feature_columns)

print("\nâœ… Les donnÃ©es sont maintenant normalisÃ©es (moyenne â‰ˆ 0, Ã©cart-type â‰ˆ 1)")

"""FONCTION D'Ã‰VALUATION"""

def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):
    """
    Ã‰value un modÃ¨le et affiche les mÃ©triques de performance
    """
    # PrÃ©dictions
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)

    # MÃ©triques Train
    train_mae = mean_absolute_error(y_train, y_train_pred)
    train_mse = mean_squared_error(y_train, y_train_pred)
    train_rmse = np.sqrt(train_mse)
    train_r2 = r2_score(y_train, y_train_pred)

    # MÃ©triques Test
    test_mae = mean_absolute_error(y_test, y_test_pred)
    test_mse = mean_squared_error(y_test, y_test_pred)
    test_rmse = np.sqrt(test_mse)
    test_r2 = r2_score(y_test, y_test_pred)

    # Affichage
    print("=" * 70)
    print(f"ðŸ“Š RÃ‰SULTATS: {model_name}")
    print("=" * 70)
    print(f"\n{'MÃ©trique':<20} {'Train':<15} {'Test':<15}")
    print("-" * 70)
    print(f"{'MAE':<20} {train_mae:<15.4f} {test_mae:<15.4f}")
    print(f"{'MSE':<20} {train_mse:<15.4f} {test_mse:<15.4f}")
    print(f"{'RMSE':<20} {train_rmse:<15.4f} {test_rmse:<15.4f}")
    print(f"{'RÂ² Score':<20} {train_r2:<15.4f} {test_r2:<15.4f}")

    return {
        'model_name': model_name,
        'train_mae': train_mae,
        'test_mae': test_mae,
        'train_r2': train_r2,
        'test_r2': test_r2,
        'test_rmse': test_rmse,
        'y_test_pred': y_test_pred
    }

print("âœ… Fonction d'Ã©valuation crÃ©Ã©e!")

"""LINEAR REGRESSION"""

# ModÃ¨le 1: Linear Regression
lr_model = LinearRegression()
lr_model.fit(X_train_scaled, y_train)

lr_results = evaluate_model(lr_model, X_train_scaled, X_test_scaled, y_train, y_test, "Linear Regression")

print("\n" + "=" * 70)
print("COEFFICIENTS DU MODÃˆLE LINEAR REGRESSION")
print("=" * 70)
coefficients = pd.DataFrame({
    'Feature': feature_columns,
    'Coefficient': lr_model.coef_
}).sort_values('Coefficient', key=abs, ascending=False)
print(coefficients)

"""RIDGE REGRESSION"""

# ModÃ¨le 2: Ridge Regression
ridge_model = Ridge(alpha=1.0, random_state=42)
ridge_model.fit(X_train_scaled, y_train)

ridge_results = evaluate_model(ridge_model, X_train_scaled, X_test_scaled, y_train, y_test, "Ridge Regression")

print("\n" + "=" * 70)
print("COEFFICIENTS DU MODÃˆLE RIDGE REGRESSION")
print("=" * 70)
ridge_coefficients = pd.DataFrame({
    'Feature': feature_columns,
    'Coefficient': ridge_model.coef_
}).sort_values('Coefficient', key=abs, ascending=False)
print(ridge_coefficients)

# ModÃ¨le 4: Decision Tree
dt_model = DecisionTreeRegressor(max_depth=10, min_samples_split=10, random_state=42)
dt_model.fit(X_train, y_train)

dt_results = evaluate_model(dt_model, X_train, X_test, y_train, y_test, "Decision Tree")

# ModÃ¨le 5: Random Forest
rf_model = RandomForestRegressor(n_estimators=100, max_depth=15, random_state=42, n_jobs=-1)
rf_model.fit(X_train, y_train)

rf_results = evaluate_model(rf_model, X_train, X_test, y_train, y_test, "Random Forest")

# ModÃ¨le 6: Gradient Boosting
gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)
gb_model.fit(X_train, y_train)

gb_results = evaluate_model(gb_model, X_train, X_test, y_train, y_test, "Gradient Boosting")

"""ðŸ“Š COMPARAISON DES MODÃˆLES"""

# ModÃ¨le 3: Lasso Regression
lasso_model = Lasso(alpha=0.1, random_state=42)
lasso_model.fit(X_train_scaled, y_train)

lasso_results = evaluate_model(lasso_model, X_train_scaled, X_test_scaled, y_train, y_test, "Lasso Regression")

print("\n" + "=" * 70)
print("COEFFICIENTS DU MODÃˆLE LASSO REGRESSION")
print("=" * 70)
lasso_coefficients = pd.DataFrame({
    'Feature': feature_columns,
    'Coefficient': lasso_model.coef_
}).sort_values('Coefficient', key=abs, ascending=False)
print(lasso_coefficients)

# CrÃ©er un tableau comparatif de tous les modÃ¨les
all_results = [lr_results, ridge_results, lasso_results, dt_results, rf_results, gb_results]

results_df = pd.DataFrame([
    {
        'ModÃ¨le': result['model_name'],
        'Train RÂ²': result['train_r2'],
        'Test RÂ²': result['test_r2'],
        'Test RMSE': result['test_rmse'],
        'Test MAE': result['test_mae']
    }
    for result in all_results
])

print("=" * 90)
print("COMPARAISON DE TOUS LES MODÃˆLES")
print("=" * 90)
print(results_df.to_string(index=False))

# Meilleur modÃ¨le
best_idx = results_df['Test RÂ²'].idxmax()
best_model_name = results_df.loc[best_idx, 'ModÃ¨le']
best_r2 = results_df.loc[best_idx, 'Test RÂ²']

print("\n" + "=" * 90)
print(f"ðŸ† MEILLEUR MODÃˆLE: {best_model_name} (RÂ² = {best_r2:.4f})")
print("=" * 90)

"""# VISUALISATION DES MODÃˆLES"""

# Graphique 1: Comparaison RÂ² Score
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# RÂ² Score
axes[0].bar(results_df['ModÃ¨le'], results_df['Test RÂ²'], color='steelblue', edgecolor='black', linewidth=2)
axes[0].set_title('RÂ² Score (Test Set)', fontsize=14, fontweight='bold')
axes[0].set_ylabel('RÂ² Score')
axes[0].tick_params(axis='x', rotation=45)
axes[0].grid(alpha=0.3, axis='y')
axes[0].axhline(y=results_df['Test RÂ²'].mean(), color='red', linestyle='--', label='Moyenne')
axes[0].legend()

# RMSE
axes[1].bar(results_df['ModÃ¨le'], results_df['Test RMSE'], color='coral', edgecolor='black', linewidth=2)
axes[1].set_title('RMSE (Test Set)', fontsize=14, fontweight='bold')
axes[1].set_ylabel('RMSE')
axes[1].tick_params(axis='x', rotation=45)
axes[1].grid(alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

print("âœ… Graphiques crÃ©Ã©s!")

"""# ANALYSE DES PRÃ‰DICTIONS DU MEILLEUR MODÃˆLE"""

# Utiliser le meilleur modÃ¨le (Random Forest gÃ©nÃ©ralement)
best_model = rf_model
y_pred_test = best_model.predict(X_test)

# Graphique 1: Valeurs rÃ©elles vs prÃ©dites
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Scatter plot
axes[0].scatter(y_test, y_pred_test, alpha=0.6, color='blue', edgecolor='black')
axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Ligne parfaite')
axes[0].set_title('Valeurs RÃ©elles vs PrÃ©dictions', fontsize=14, fontweight='bold')
axes[0].set_xlabel('Valeurs RÃ©elles (Units_Sold)')
axes[0].set_ylabel('Valeurs PrÃ©dites')
axes[0].grid(alpha=0.3)
axes[0].legend()

# RÃ©sidus
residuals = y_test - y_pred_test
axes[1].scatter(y_pred_test, residuals, alpha=0.6, color='purple', edgecolor='black')
axes[1].axhline(y=0, color='r', linestyle='--', lw=2)
axes[1].set_title('Analyse des RÃ©sidus', fontsize=14, fontweight='bold')
axes[1].set_xlabel('Valeurs PrÃ©dites')
axes[1].set_ylabel('RÃ©sidus (RÃ©el - PrÃ©dit)')
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print("âœ… Graphiques d'analyse crÃ©Ã©s!")

"""# IMPORTANCE DES FEATURES (Random Forest)"""

# Importance des features
feature_importance = pd.DataFrame({
    'Feature': feature_columns,
    'Importance': rf_model.feature_importances_
}).sort_values('Importance', ascending=False)

print("=" * 70)
print("IMPORTANCE DES FEATURES")
print("=" * 70)
print(feature_importance.to_string(index=False))

# Visualisation
plt.figure(figsize=(12, 8))
plt.barh(feature_importance['Feature'], feature_importance['Importance'],
         color='teal', edgecolor='black', linewidth=2)
plt.title('Importance des Features (Random Forest)', fontsize=16, fontweight='bold')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.gca().invert_yaxis()
plt.grid(alpha=0.3, axis='x')
plt.tight_layout()
plt.show()

print("\nâœ… Graphique crÃ©Ã©!")

print("=" * 80)
print("ðŸ“Š RÃ‰SUMÃ‰ COMPLET DE L'ANALYSE E-COMMERCE")
print("=" * 80)

print(f"""
âœ… DATASET ANALYSÃ‰
   â€¢ Nombre de transactions: {df.shape[0]}
   â€¢ PÃ©riode: {df['Date'].min().strftime('%d/%m/%Y')} Ã  {df['Date'].max().strftime('%d/%m/%Y')}
   â€¢ Nombre de features: {len(feature_columns)}

ðŸ’° STATISTIQUES FINANCIÃˆRES
   â€¢ Revenu total: ${df['Sales'].sum():,.2f}
   â€¢ Revenu moyen: ${df['Sales'].mean():,.2f}
   â€¢ UnitÃ©s vendues: {df['Units_Sold'].sum()} unitÃ©s

ðŸ† MEILLEUR MODÃˆLE
   â€¢ ModÃ¨le: {best_model_name}
   â€¢ RÂ² Score (Train): {results_df.loc[best_idx, 'Train RÂ²']:.4f}
   â€¢ RÂ² Score (Test): {results_df.loc[best_idx, 'Test RÂ²']:.4f}
   â€¢ RMSE (Test): {results_df.loc[best_idx, 'Test RMSE']:.4f}

ðŸ“Œ INSIGHTS CLÃ‰S
   â€¢ Variable la plus importante: {feature_importance.iloc[0]['Feature']} ({feature_importance.iloc[0]['Importance']:.4f})
   â€¢ CorrÃ©lation Prix-Ventes: {df['Price'].corr(df['Units_Sold']):.4f}
   â€¢ CorrÃ©lation Marketing-Ventes: {df['Marketing_Spend'].corr(df['Units_Sold']):.4f}
   â€¢ CorrÃ©lation RÃ©duction-Ventes: {df['Discount'].corr(df['Units_Sold']):.4f}

âœ¨ CATÃ‰GORIES LES PLUS RENTABLES
""")

top_categories = df.groupby('Product_Category')['Sales'].mean().sort_values(ascending=False)
for cat, sales in top_categories.items():
    print(f"   â€¢ {cat}: ${sales:,.2f}")

print(f"""
ðŸ‘¥ SEGMENTS CLIENTS LES PLUS RENTABLES
""")

top_segments = df.groupby('Customer_Segment')['Sales'].mean().sort_values(ascending=False)
for seg, sales in top_segments.items():
    print(f"   â€¢ {seg}: ${sales:,.2f}")

print("\n" + "=" * 80)
print("ðŸŽ‰ ANALYSE COMPLÃˆTE TERMINÃ‰E !")
print("=" * 80)

"""SAUVEGARDE DU MODÃˆLE"""

import pickle

# Sauvegarder le modÃ¨le
with open('rf_model.pkl', 'wb') as f:
    pickle.dump(rf_model, f)

print("âœ… ModÃ¨le sauvegardÃ©!")